---
title: "Coding Assignment 3"
author: "Team 4"
date: "Due: 2024-12-07 23:59"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
library(readxl) # reading in excel file
library(car) # for vif function
library(plotly) # for interactive visualizations
library(gt) # for better looking tables
library(gtsummary) # for better summary statistics
knitr::opts_chunk$set(echo = TRUE)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 100 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 100 customers in the sample is as follows:

-	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r dataset}
# Bring in the dataset here.
#insurance_data <- read.csv("~/Documents/GitHub/ECO6416_Group04/Insurance_Data_Group4.csv"

insurance <- read.csv("~/Documents/GitHub/ECO6416_Group04/Insurance_Data_Group4.csv")

```


## Question 1

Randomly select 30 observations from the sample and exclude from all modeling. Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 70 observations.

```{r q1}
#
set.seed(123)
index <- sample(seq_len(nrow(insurance)), size = 30)
train <- insurance[-index,]
test <- insurance[index,]

train %>% 
  tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
                                                    "{median} ({p25}, {p75})",
                                                    "{min}, {max}"),
                              all_categorical() ~ "{n} / {N} ({p}%)"),
              type = all_continuous() ~ "continuous2"
  )
```


## Question 2

Provide the correlation between all quantitative variables

```{r}
included_data <- train %>% select(Charges, Age, BMI, Children)
cor(included_data)
```
Charges and Age have the strongest correlation out of all the variables. All of the rest of the variables (Charges & BMI, Charges & Children, Age & BMI, Age & Children, and BMI & Children) have weak positive correlation. 

## Question 3 

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?

```{r}
model <- lm(Charges ~ Age + BMI + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = insurance)
sum<-summary(model)
sum
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(model)

dwtest(model)
```
The model above violates homoscedasticity, perfect collinearity, and independence. The solution to correcting the heteroscedasticity is using different weightings in least squares. The solution to correcting the multicollinearity is by checking tthe VIF and removing or combining explanatory variables. The solution to correcting the autocorrelation is adding lag variables to see if there's a difference over time periods. 

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4}
# Log Charges and Charges Squared 
train$LogCharges <- log(train$Charges)
train$ChargesSquared <- train$Charges^2

# Log Age and Age Squared 
train$LogAge <- log(train$Age)
train$AgeSquared <- train$Age^2

# Log Smoker and Smoker Squares 
train$lnSmoker <- log(train$Smoker)
train$SmokerSquared <- train$Smoker^2

# Log BMI and BMI Squared 
train$LogBMI <- log(train$BMI)
train$BMISquared <- train$BMI^2

# Interaction: Children and Age
model_1 <- lm(LogCharges ~LogAge + Children, data = train)
summary(model_1)

# Interaction: Smoker and Female
model_2 <- lm(LogCharges ~ SmokerSquared + Female, data = train)
summary(model_2)

# Interaction: BMI and Oviedo
model_3 <- lm(LogCharges ~train$LogBMI + Oviedo, data = train)
summary(model_3)

# Regression without Children, Female
model_4 <- lm(Charges ~ Age + BMI + Smoker + WinterSprings + WinterPark + Oviedo, data = insurance)
summary(model_4)

# Regression without Age, Smoker
model_5 <- lm(Charges ~ BMI + Children + Female + WinterSprings + WinterPark + Oviedo, data = insurance)
summary(model_5)
#
```

Models 1 and 2 have an improved fit in comparison to the original regression. 

## Question 5

Use the 30 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}
test$LogCharges <- log(test$Charges)
test$LogAge <- log(test$Age)
test$SmokerSquared <- test$Smoker^2

test$model_1_pred <- predict(model_1, newdata= test) %>% exp ()
test$model_2_pred <- predict(model_2, newdata= test) %>% exp ()

test$error_1 <- test$model_1_pred - test$Charges
test$error_2 <- test$model_2_pred - test$Charges

mae <- function(error_vector){error_vector %>%  abs() %>%  mean()}

rmse <- function(error_vector){
   error_vector^2 %>%  mean() %>% sqrt()}

mape <- function(error_vector, actual_vector){
  (error_vector/actual_vector) %>% 
    abs() %>% 
    mean()
}

bias <- mean(test$error_1)
print("Mean for Model 1")
print(bias)
mae <- mae(test$error_1)
print("MAE for Model 1")
print(mae)
mape <- mape(test$error_1, test$Charges)
print("MAPE for Model 1")
print(mape)
rmse <- rmse(test$error_1)
print("RMSE for Model 1")
print(rmse)

bias <- mean(test$error_2)
print("Mean for Model 2")
print(bias)
mae <- mae(test$Charges, test$model_2_pred)
print("MAE for Model 2")
print(mae)
mape <- mape(test$error_2, test$Charges)
print("MAPE for Model 2")
print(mape)
rmse <- rmse(test$Charges, test$model_2_pred)
print("RMSE for Model 2")
print(rmse)
```
Long-term, the better model overall is Model 2. Model 2 has significantly lower MAPE and RMSE, which shows a higher level of accuracy and consistency. Both models show bias, but lower MAE, MAPE, and RMSE for Model 2 show that over time it has more accurate predictions.

## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r}
coef_model1 <- c(Intercept = 4.42138, LogAge = 1.26023, Children = 0.08646)

marginal_change_model1 <- list(
  LogAge = (exp(coef_model1["LogAge"]) - 1) * 100,   # Percent change for LogAge
  Children = (exp(coef_model1["Children"]) - 1) * 100  # Percent change for Children
)

print("Marginal Change Analysis for Model 1:")
print(marginal_change_model1)

coef_model2 <- c(Intercept = 8.74332, SmokerSquared = 1.55813, Female = 0.06303)

marginal_change_model2 <- list(
  SmokerSquared = (exp(coef_model2["SmokerSquared"]) - 1) * 100,  # Percent change for SmokerSquared
  Female = (exp(coef_model2["Female"]) - 1) * 100  # Percent change for Female
)

print("Marginal Change Analysis for Model 2:")
print(marginal_change_model2)
```


## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
| -------- | --- | --- | ------ | -------- | ------ | -------------- | 
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |


```{r}
new_clients <- data.frame(
  Age = c(60, 40, 25, 33, 45),
  BMI = c(22, 30, 25, 35, 27),
  Female = c(1, 0, 0, 1, 1),
  Children = c(0, 1, 0, 2, 3),
  Smoker = c(0, 0, 1, 0, 0),
  City = c("Oviedo", "Sanford", "Winter Park", "Winter Springs", "Oviedo")
)

new_clients$LogAge <- log(new_clients$Age)
new_clients$SmokerSquared <- new_clients$Smoker^2
new_clients$LogCharges

new_clients$model_2_pred <- predict(model_2, newdata = new_clients)

new_clients$lower_PI <- new_clients$model_2_pred - 1.96 * sd(new_clients$model_2_pred)
new_clients$upper_PI <- new_clients$model_2_pred + 1.96 * sd(new_clients$model_2_pred)

print(new_clients)
```


## Question 8
The owner notices that some of the predictions are wider than others, explain why.

Some of the predictions are wider than others because customers whose features (Age, BMI, etc.) that are farther from the average data in the training set will have wider prediction intervals due to higher uncertainty in the model’s predictions. On the other hand, predictions that are more typical of the training data will have narrower intervals because the model can predict these values with greater confidence.

## Question 9 
Are there any prediction problems that occur with the five potential clients? If so, explain.

Yes, a couple of the prediction problems that could occur are multicollinearity because Age and BMI are highly correlated, as older individuals may be more likely to have higher BMI. Since these variables are highly correlated, the model may have difficulty isolating their individual effects on the dependent variable. Another prediction problem could be outliers like age for customer one or BMI for customer 4. If the model hasn't learned how to predict for such outliers, it could result in poor predictions for this customer.